{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d14c39e7-8d6a-4fec-b6da-46d171362050",
   "metadata": {},
   "source": [
    "## Climate Change and the Optimal Decisions of Vineyard Farmers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249d9e79-3ebc-464e-b7ab-146c9f61e178",
   "metadata": {},
   "source": [
    "#### Background\n",
    "\n",
    "Wine industry is highly sensitive to the nuances of climate and terroir, is facing unprecedented challenges due to climate change. And climate change will affect vineyards production through shifts in temperature and precipitation patterns. \n",
    "\n",
    "One response to the climatic shift is \"migration\" -- relocate to a more climatically suitable area. But this strategy might incurr a potential loss of location-specific premium associated with established wine-producing regions.\n",
    "\n",
    "Another possible stretagy is \"adaptation\". This includes introducing new grape varieties, using new techniques, rethinking vineyard orientations, etc. And this will increase the cost of production. \n",
    "\n",
    "#### Problem Statement\n",
    "\n",
    "We aim to employ a Reinforcement Learning (RL) model to optimize strategies in the context of climate change. The model is designed to operate under varying temperature scenarios, with each scenario influencing the vineyard's state and the effectiveness of potential adaptation actions.  \n",
    "\n",
    "The RL model framework involves:\n",
    "\n",
    "__1.Space State ($S$):__\n",
    "Defined by a range of temperature scenarios and other relevant vineyard conditions. Each state $s\\in S$ represents specific environmental and climatic conditions.\n",
    "\n",
    "__2.Action Space($A$):__\n",
    "\n",
    "__3.Object Function:__\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\text{minimize}~\\mathcal{C}_T &=& \\sum{i\\in{1,\\dotsc,n}}c_{i}x_{i} \\\\\n",
    "\\text{subject to}~\\sum_{i\\in{1,\\dotsc,n}}p_{i}x_{i} & \\leq & P_{\\text{max}}\\\\\n",
    "\\text{and}~\\mathbf{C}\\mathbf{x} & \\leq & \\mathbf{b} \\\\\n",
    "\\text{and}~x_{i}&\\in&{0,1}\\qquad{i=1,2,\\dots,n}\n",
    "\\end{eqnarray}\n",
    "\n",
    "In this model:\n",
    "\n",
    "* $c_{i}\\geq 0$ denotes the cost of adopting strategy$i$. \n",
    "* $p_{i}$ denotes the potential impact on the location premium by adopting strategy$i$, with $P_{max}$ representing the maximum permissible impact on the premium.\n",
    "* $x_{i}\\in{0,1}$ represents the binary decision of adopting or not adopting strategy$i$. \n",
    "\n",
    "\n",
    "#### List of Tasks\n",
    "* __Task 1__: Specify the vectors\n",
    "* __Task 2__: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebba332-bc86-43a5-b1ff-f0cbd11213bf",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c70b150f-24ba-468d-b6f3-86c074b2583d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "include(\"CHEME-5760-L13a-CodeLib.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "188f5b57-d428-458e-a891-51a2c0690d44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using Plots\n",
    "using Colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a24cc9a-2fcc-40ab-86da-2efcdebef9c6",
   "metadata": {},
   "source": [
    "## Task 1: Specify the Vectors\n",
    "\n",
    "In this problem, we will choose Pinot Grins as baseline.\n",
    "\n",
    "The optimal temperature for Pinot Grins to grow is between 13-15 celsius degree (55.4 - 59 Fahrenheit). \n",
    "\n",
    "There are several strategies can be taken\n",
    "\n",
    "### Adaptation Strategies\n",
    "\n",
    "__1.__ Adjust harvest dates\n",
    "\n",
    "* Estimated Cost: $0 per acre\n",
    "* Temperature Offset: 1 per 14 days - but date adjustment cannot be larger than 30 days\n",
    "\n",
    "__2.__ Switch to another cultivar (assume from Pinot Grins to Cabernet Sauvignon)\n",
    "\n",
    "* Estimated Cost: $824.6 per acre (the yield per harvested acre for Pinot Grins is 4.74, and price pre ton is 1800 -> 8532 per acre; the yield per harvested acre for Pinot Grins is 3.01, and price pre ton is 2560 -> 7705.6 per acre)\n",
    "* Temperature Offset: 9 the optimal temperature for Cabernet Sauvignon is 16.5-20 (61.7-68)\n",
    "\n",
    "__3.__ Production technology -- full-capacity watering\n",
    "\n",
    "* Estimated Cost: 500-1200 USD per acre\n",
    "* Temperature Offset: 2-4\n",
    "\n",
    "__4.__ Production technology -- canopy manipulation\n",
    "\n",
    "* Estimated Cost: 300-800 USD per acre\n",
    "* Temperature Offset: 1-3\n",
    "\n",
    "__5.__ Production technology -- changing row orientation\n",
    "\n",
    "* Estimated Cost: 5000-10000 USD per acre (one-time cost)\n",
    "* Temperature Offset: 2-4\n",
    "\n",
    "\n",
    "### Migration\n",
    "\n",
    "__6.__ Northward movement\n",
    "\n",
    "* Estimated Cost (premium loss): $6930 per acre\n",
    "* Temperature Offset: will reach the optimal temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53ade3c1-1e85-4101-a977-a6c012d057f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Int64, String} with 6 entries:\n",
       "  5 => \"ChangeRowOrientation\"\n",
       "  4 => \"CanopyManipulation\"\n",
       "  6 => \"NorthwardMovement\"\n",
       "  2 => \"ChangeVineType\"\n",
       "  3 => \"FullCapacityWatering\"\n",
       "  1 => \"HarvestDate\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a model of the states (temperature)\n",
    "number_of_rows = 121 # temperature range from 0 - 120 fahrenheit\n",
    "number_of_cols = 121\n",
    "\n",
    "nstates = (number_of_rows*number_of_cols);\n",
    "min_temp = 0\n",
    "max_temp = 120\n",
    "min_temp_range = 0:1:120\n",
    "max_temp_range = 0:1:120 \n",
    "ùíÆ = [(min_temp, max_temp) for min_temp in min_temp_range for max_temp in max_temp_range if min_temp <= max_temp]\n",
    "\n",
    "nactions = 6\n",
    "ùíú = 1:nactions\n",
    "action_mapping = Dict(1 => \"HarvestDate\", 2 => \"ChangeVineType\", 3 => \"FullCapacityWatering\", \n",
    "                      4 => \"CanopyManipulation\", 5 => \"ChangeRowOrientation\", 6 => \"NorthwardMovement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "989fd011-965e-4249-bc14-96a6a079ebcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define costs and temperature offsets\n",
    "action_costs = [0.00, 824.60, 750.00, 550.00, 7500.00, 6930.00] \n",
    "temperature_offsets = [0.07, 9.00, 3.00, 2.00, 3.00, 100.00]\n",
    "\n",
    "default_reward = 0.00\n",
    "\n",
    "current_temp_min = 60\n",
    "current_temp_max = 70\n",
    "\n",
    "optimal_temp_min = 55\n",
    "optimal_temp_max = 59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee64e66b-0b27-41aa-8581-99732d809930",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Set{Tuple{Int64, Int64}} with 1 element:\n",
       "  (55, 59)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define specific rewards\n",
    "optimal_condition_reward = 10000.0\n",
    "suboptimal_condition_reward = -500.0\n",
    "\n",
    "# function to adjust rewards based on strategy cost and temperature offset\n",
    "function adjust_reward(state::Tuple{Int, Int}, action::Int)\n",
    "    cost = action_costs[action]\n",
    "\n",
    "    # Determine if the state meets the optimal temperature criteria\n",
    "    if state[1] <= optimal_temp_max || state[2] >= optimal_temp_min\n",
    "        base_reward = optimal_condition_reward\n",
    "    else\n",
    "        base_reward = suboptimal_condition_reward\n",
    "    end\n",
    "\n",
    "    # Calculate the final reward\n",
    "    final_reward = base_reward - cost\n",
    "    return final_reward\n",
    "end\n",
    "\n",
    "# set up rewards\n",
    "rewards = Dict{Tuple{Tuple{Int, Int}, Int}, Float64}()\n",
    "for state in ùíÆ\n",
    "    for action in ùíú\n",
    "        reward = adjust_reward(state, action)\n",
    "        rewards[(state, action)] = reward\n",
    "    end\n",
    "end\n",
    "\n",
    "# set up absorbing state\n",
    "absorbing_state_set = Set{Tuple{Int, Int}}()\n",
    "push!(absorbing_state_set, (optimal_temp_min,optimal_temp_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef6e0e3d-4004-4183-bb2e-4c3270fe9423",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Tuple{Tuple{Int64, Int64}, Int64}, Float64} with 44286 entries:\n",
       "  ((6, 49), 3)    => 9250.0\n",
       "  ((52, 94), 3)   => 9250.0\n",
       "  ((10, 81), 1)   => 10000.0\n",
       "  ((29, 104), 1)  => 10000.0\n",
       "  ((0, 21), 3)    => 9250.0\n",
       "  ((34, 92), 3)   => 9250.0\n",
       "  ((24, 39), 6)   => 3070.0\n",
       "  ((13, 33), 3)   => 9250.0\n",
       "  ((6, 15), 2)    => 9175.4\n",
       "  ((19, 67), 5)   => 2500.0\n",
       "  ((93, 118), 5)  => 2500.0\n",
       "  ((58, 69), 2)   => 9175.4\n",
       "  ((103, 103), 1) => 10000.0\n",
       "  ((12, 99), 6)   => 3070.0\n",
       "  ((31, 58), 6)   => 3070.0\n",
       "  ((13, 58), 2)   => 9175.4\n",
       "  ((17, 115), 5)  => 2500.0\n",
       "  ((19, 62), 6)   => 3070.0\n",
       "  ((38, 45), 2)   => 9175.4\n",
       "  ((26, 94), 5)   => 2500.0\n",
       "  ((41, 112), 6)  => 3070.0\n",
       "  ((108, 114), 1) => 10000.0\n",
       "  ((68, 115), 6)  => 3070.0\n",
       "  ((23, 33), 5)   => 2500.0\n",
       "  ((62, 108), 6)  => 3070.0\n",
       "  ‚ãÆ               => ‚ãÆ"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37113a23-05ac-4c0b-b1cb-0b351ef95803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reward shaping\n",
    "is_reward_shaping_on = true;\n",
    "\n",
    "if (is_reward_shaping_on == true)\n",
    "    for state in ùíÆ\n",
    "        for action in ùíú\n",
    "           # Create a state-action pair as a coordinate\n",
    "            state_action_pair = (state, action)\n",
    "        \n",
    "            # Check and update the reward for each state-action pair\n",
    "            if !haskey(rewards, state_action_pair) && !in(state, absorbing_state_set)\n",
    "                rewards[state_action_pair] = adjust_reward(state, action)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaceca0c-64a0-4dca-ba25-6e21e9c339cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "world_model = build(MyRectangularGridWorldModel, (\n",
    "        nrows=number_of_rows, ncols=number_of_cols, rewards = rewards, actions = ùíú ));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0d3499d-080c-460c-88e9-c367e3110a63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyRectangularGridWorldModel(121, 121, Dict{Int64, Tuple{Int64, Int64}}(), Dict{Tuple{Int64, Int64}, Int64}(), Dict(5 => (-3, -3), 4 => (-2, -2), 6 => (-5, -11), 2 => (-9, -9), 3 => (-3, -3), 1 => (-1, -1)), Dict(((6, 49), 3) => 9250.0, ((52, 94), 3) => 9250.0, ((10, 81), 1) => 10000.0, ((29, 104), 1) => 10000.0, ((0, 21), 3) => 9250.0, ((34, 92), 3) => 9250.0, ((24, 39), 6) => 3070.0, ((13, 33), 3) => 9250.0, ((6, 15), 2) => 9175.4, ((19, 67), 5) => 2500.0‚Ä¶))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11e5846-a665-45ed-889e-5a08e8b713a3",
   "metadata": {},
   "source": [
    "## Task 2: Q-learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9da8a90-28ca-4565-b55b-4f82043d7481",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Œ± = 0.7;  # learning rate\n",
    "Œ≥ = 0.95; # discount rate\n",
    "nstates = (number_of_rows*number_of_cols);\n",
    "agent_model = build(MyQLearningAgentModel, (\n",
    "    states = ùíÆ,\n",
    "    actions = ùíú,\n",
    "    Œ± = Œ±,\n",
    "    Œ≥ = Œ≥,\n",
    "    Q = zeros(nstates,nactions)\n",
    "));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13711e5c-480a-4e73-917b-aae4273e251b",
   "metadata": {},
   "source": [
    "## Task 3: Simulate and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc9c96d8-fea4-4082-ae28-ee101309ca5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "startstate = (current_temp_min,current_temp_max); # start position\n",
    "number_of_episodes = 200;\n",
    "number_of_iterations = 1000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f3bdd69-3d8b-4510-ab64-df5308ea48f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running episode 1\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "KeyError: key (60, 70) not found",
     "output_type": "error",
     "traceback": [
      "KeyError: key (60, 70) not found",
      "",
      "Stacktrace:",
      " [1] getindex",
      "   @ ./dict.jl:484 [inlined]",
      " [2] simulate(agent::MyQLearningAgentModel, environment::MyRectangularGridWorldModel, startstate::Tuple{Int64, Int64}, maxsteps::Int64; œµ::Float64)",
      "   @ Main ~/Documents/GitHub/Climate Change and Vineyard/CHEME-5760-L13a-CodeLib.jl:229",
      " [3] top-level scope",
      "   @ ./In[19]:6"
     ]
    }
   ],
   "source": [
    "my_Q_dictionary = Dict{Tuple{Int,Int}, Array{Float64,2}}();\n",
    "coordinate = startstate;\n",
    "for i ‚àà 1:number_of_episodes\n",
    "    println(\"Running episode $i\")\n",
    "    # run an episode, and grab the Q\n",
    "    result = simulate(agent_model, world_model, coordinate, number_of_iterations, œµ = 0.7);\n",
    "    println(\"Running episode $i\")\n",
    "    agent_model.Q = result.Q;\n",
    "end\n",
    "my_Q_dictionary[coordinate] = agent_model.Q;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80380319-67df-4e06-a26e-5faec7ff5826",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Tuple{Int64, Int64}, Matrix{Float64}}()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_Q_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60ee0309-4dd1-48d3-bcde-4d2be15a3791",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "KeyError: key (60, 70) not found",
     "output_type": "error",
     "traceback": [
      "KeyError: key (60, 70) not found",
      "",
      "Stacktrace:",
      " [1] getindex(h::Dict{Tuple{Int64, Int64}, Matrix{Float64}}, key::Tuple{Int64, Int64})",
      "   @ Base ./dict.jl:484",
      " [2] top-level scope",
      "   @ In[17]:1"
     ]
    }
   ],
   "source": [
    "Q = my_Q_dictionary[startstate];\n",
    "my_œÄ = policy(Q);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95969c67-faec-40e7-be6f-c480698ffc41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `Q` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `Q` not defined",
      ""
     ]
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6049afcd-a56c-4690-ad89-58255aa97560",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "KeyError: key (60, 70) not found",
     "output_type": "error",
     "traceback": [
      "KeyError: key (60, 70) not found",
      "",
      "Stacktrace:",
      " [1] getindex(h::Dict{Tuple{Int64, Int64}, Int64}, key::Tuple{Int64, Int64})",
      "   @ Base ./dict.jl:484",
      " [2] top-level scope",
      "   @ In[15]:5"
     ]
    }
   ],
   "source": [
    "# draw the path -\n",
    "p = plot();\n",
    "initial_site = startstate\n",
    "hit_absorbing_state = false\n",
    "s = world_model.states[initial_site];\n",
    "visited_sites = Set{Tuple{Int,Int}}();\n",
    "push!(visited_sites, initial_site);\n",
    "\n",
    "while (hit_absorbing_state == false)\n",
    "    current_position = world_model.coordinates[s]\n",
    "    a = my_œÄ[s];\n",
    "    Œî = world_model.moves[a];\n",
    "    new_position =  current_position .+ Œî\n",
    "    scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, msc=:black, c=:blue)\n",
    "    plot!([current_position[1], new_position[1]],[current_position[2],new_position[2]], label=\"\", arrow=true, lw=1, c=:gray)\n",
    "    \n",
    "    if (in(new_position, absorbing_state_set) == true || in(new_position, visited_sites) == true)\n",
    "        hit_absorbing_state = true;\n",
    "    elseif (haskey(world_model.states, new_position) == true)\n",
    "        s = world_model.states[new_position];\n",
    "        push!(visited_sites, new_position);\n",
    "    else\n",
    "        hit_absorbing_state = true; # we drove off the map\n",
    "    end\n",
    "end\n",
    "\n",
    "# draw the grid -\n",
    "for s ‚àà ùíÆ\n",
    "    current_position = world_model.coordinates[s]\n",
    "    a = my_œÄ[s];\n",
    "    Œî = world_model.moves[a];\n",
    "    new_position =  current_position .+ Œî\n",
    "    \n",
    "    if (haskey(rewards, current_position) == true && rewards[current_position] == charging_reward)\n",
    "        scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, c=:green, ms=4)\n",
    "    elseif (haskey(rewards, current_position) == true && rewards[current_position] == lava_reward)\n",
    "        scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, c=:red, ms=4)\n",
    "    elseif (in(current_position, soft_wall_set) == true)\n",
    "        scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, c=:gray69, ms=4)\n",
    "    else\n",
    "        if (is_reward_shaping_on == true)\n",
    "            new_color = weighted_color_mean(rbf(current_position, (5,6), œÉ = œÉ), colorant\"green\", colorant\"white\")\n",
    "            scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, msc=:lightgray, c=new_color)\n",
    "        else\n",
    "            scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, msc=:black, c=:white)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "current()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
